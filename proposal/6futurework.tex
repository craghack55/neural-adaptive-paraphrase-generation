\chapter{Future Work}\label{conclusion}

There are many research questions regarding adaptive neural models for NLP which are not addressed in this thesis due to the time limitations. This section gives an overview of the future work which can address the remaining research questions. 

The experiments that are done in the thesis only simulates a human-in-the-loop data acquisition process. Since the only part that can be simulated is the iterative, continuous nature of the process, the actual human input on model's predictions are not utilized in the experiments. First and most important future work would be integrating the neural model to a real word human-in-the-loop data acquisition tool like \cite{par4sim}. It is important to study the impact of interactive human evaluation and correction on the model. 

Parallel to the first future work, a human evaluation of the paraphrases that are generated by the model, is also important. Considering the fact the BLEU is an automatic metric which is not designed for paraphrase generation, incorporating the human evaluation of the suggested paraphrases could be helpful to decide when to stop the data acquisition. Extending the user interface to provide exactly that would be an improvement to the data acquisition tool.

As it is stated in the first chapter, when paraphrasing source sentences, the users choose from a list of paraphrase suggestions created by the model and external resources. With this way it is ensured that the paraphrases created by human users come from a vocabulary which is known by the model. Even though it introduces restrictions to the users, this has to be done since the model cannot change it's vocabulary after the training is started. To tackle this problem, very large pre-trained word embeddings like GloVe \cite{glove} or Word2Vec Google News \cite{mikolov} can be used. This can improve the performance and make the model more robust.

Regardless the way of how the vocabulary is created, there will always be some words that are not in the vocabulary. Especially in a setting where the training data is constantly streamed into the model, out of the vocabulary words are inevitable. A mechanism to intelligently handle out of vocabulary words, like \cite{ofw} , should be integrated into the whole data acquisition process.

Transferring from different datasets in the same domain is not always effective especially if the target dataset is large enough. This assertion can be false when the transfer learning is done from different tasks as it is demonstrated in \cite{brad}. As future work the effect of transferring from different tasks for example entailment generation, can be investigated. This might be an important improvement on the current framework since it might be not possible to find a different dataset in the same domain if the domain is very specific.

Finally, since the research on active learning for paraphrase generation is non existent it is a natural future work for this thesis. The sampling methods which are experimented on in this thesis are shown to be unsuccessful therefore new sampling methods for neural paraphrase generation is still an open research question.