\chapter{Results}\label{results}

This chapter reports the results of experiments done with the proposed learning strategies and discusses the findings. First, evaluations of each strategy are given separately followed by the experiments which evaluate the combinations of different strategies. At the end, an overall discussion of the results is given.

\begin{table}[t]
\centering
\small
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  IL1 & 0.1 &  \textbf{0.92} &  \textbf{0.94} &  \textbf{1.56} &  \textbf{1.13} &  \textbf{1.11} &  \textbf{1.26} &  \textbf{1.37} &  \textbf{1.41}  \\ 
 \hline
  IL2 & 0.07 & 0.15 & 0.21 & 0.47 & 0.77 & 0.99 & 1.04 & 1.19 & 1.16 \\ 
 \hline
 IL3 & \textbf{0.12} & 0.27 & 0.6 & 1.32 & 0.96 & 1.08 & 1.24 & 1.26 & 1.34 \\ 
 \hline
 SL & - & - & - & - & - & - & - & - & 0.09  \\ 
 \hline
\end{tabular}
\caption{BLEU scores of incremental learning on the MSR dataset. Each column represents an iteration, first row shows what percent of the dataset is used in that iteration. Each row except the last represents the performance of a particular learning strategy which is described in \ref{incs}. The last row represents the results from \cite{brad}.}
\label{table:4.1}
\end{table}

\section{Neural Paraphrase Generation with Incremental Learning}

Table 4.1 presents the results of incremental learning strategies on the MSR dataset. The model is highly sensitive to parameter tuning, failing to converge with high learning rates. The model is trained on a validation set with different hyperparameter configurations, mainly with different dropout probabilities and learning rates. The configuration with the best performance is used in the experiments. This parameter tuning is done with supervised learning. As it can be seen from the table, the model performs very poorly, achieving a BLEU score of 1.34 with traditional supervised learning. This poor performance might be related to the dataset's small size and high complexity. Moreover a result from \cite{brad} is also reported in order to reinforce the difficulty of MSR dataset. Their model achieved a BLEU score of 0.09 on the MSR dataset with supervised learning. 

Incremental learning with and without data pooling achieve BLEU scores of 1.41 and 1.16 respectively. It can be seen that all of the learning strategies including the baseline, seem to improve over time, increasing their performance in a steady fashion at every iteration even though there are anomalies in baseline and incremental learning with data pooling, at the 4th iteration with 50 percent of the data. It is also observed that when trained with data pooling in an incremental manner, the model performs slightly better than supervised learning. Nevertheless the model fails to generate meaningful and grammatically correct paraphrases in every case.

Even though a clear pattern through iterations can be seen, the results of this experiment is not conclusive due to model's very poor performance. Moreover the high variance between different runs highly suggests that the results are not meaningful and the neural network is basically modelling noise. Each learning strategy has been run 3 times and the variance of the best performing learning strategy which is the incremental learning with data pooling, is 0.617. This a very high number compared to actual BLEU scores achieved by the model. 

At the end it is clear that generating coherent paraphrases for MSR dataset is impractical if it is not impossible for our model. Moreover it is not applicable to human-in-the-loop setting since it basically produces noise in early iterations. This severely undermines the involvement of model with the data acquisition process, in other words the model is unable to make meaningful suggestions to the user.

\begin{table}[b]
\centering
\small
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  IL1 & 10.49 &  \textbf{19.43} & \textbf{21.91} &  \textbf{23.30} &  \textbf{24.30} &  \textbf{24.50} &  \textbf{25.15} &  \textbf{25.45} &  \textbf{26.19}  \\ 
 \hline
  IL2 &  \textbf{10.68} & 17.58 & 18.79 & 19.06 & 19.50 & 19.46 & 19.35 & 19.54 & 19.70 \\ 
 \hline
 IL3 & 7.93 & 15.37 & 17.58 & 19.73 & 21.20 & 21.63 & 22.49 & 23.12 & 23.43 \\ 
 \hline
 SL & - & - & - & - & - & - & - & - & 22.90  \\ 
 \hline
\end{tabular}
\caption{BLEU scores of incremental learning on the QUORA dataset. Each column represents an iteration, first row shows what percent of the dataset is used in that iteration. Each row except the last represents the performance of a particular learning strategy which is described in \ref{incs}. The last row represents the results from \cite{Guptaetal}.} \label{table:incrementalQuora}
\end{table}

Table 4.2 presents the results of incremental learning strategies on the QUORA dataset. Other than a small grid search on the validation dataset with supervised learning, no hyperparameter tuning is done on the model during this experiment. In the table, IL1 represents incremental learning with data pooling where the model is trained continuously with all the data gathered by the system. IL2 represents incremental learning without data pooling where the model is continuously trained with the data gathered in the latest iteration of the system. IL3 represents the baseline incremental learning where the model is retrained from zero for each iteration. As it can be seen from the table, in every case the model performs reasonably well at the end of last iteration. The baseline model with traditional supervised learning achieves a BLEU score of 23.43 and it is able to generate meaningful, mostly grammatically correct paraphrases. The result from \cite{Guptaetal} is also reported. Their model achieved a BLEU score of 22.9 on the QUORA dataset with supervised learning. It is important to state that the training set used in this experiment is slightly larger than the reference work. Additionally a much larger test set is used in this work. Therefore it is not possible to make conclusive statements regarding the comparison of two models. Nevertheless the comparison at least shows that the model performs comparably to the other state-of-the-art approaches.

\begin{table}[t]
\centering
\small
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  0 & \textbf{10.68} & 19.13 & 20.45 & 20.83 & 20.44 & 20.47 & 20.78 & 20.31 & 21.11  \\ 
 \hline
  0.3 & 10.49 & \textbf{19.43} & \textbf{21.91} & \textbf{23.30} & \textbf{24.30} & \textbf{24.50} & \textbf{25.15} & \textbf{25.45} & \textbf{26.19} \\ 
 \hline
  0.5 & 8.48 & 17.05 & 19.62 & 21.58 & 22.93 & 23.26 & 23.92 & 24.58 & 25.09 \\ 
 \hline
\end{tabular}
\caption{BLEU scores of incremental learning with data pooling on the QUORA dataset with different dropout probabilities. Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. First column represents the dropout probabilities. The models are evaluated on the test set.}
\end{table}

Incremental learning with data pooling achieves a BLEU score of 26.19 at the last iteration, outperforming other strategies including supervised learning. Moreover it starts outperforming traditional supervised learning which uses all of the dataset, only with 60 percent of the dataset. The model starts adapting and improves its performance constantly through iterations. Even with using only half of the dataset, the model can provide reasonable paraphrases. 

The comparison between baseline incremental learning and incremental learning with data pooling can indicate the reasons why data pooling performs better. From the perspective of individual iterations with both learning strategies, the model trains with the same number of epochs and same amount of data. Only difference between two learning strategies is, in incremental learning with data pooling the model employs transfer learning inside the same dataset by not reinitializing its weights in every iteration contrary to baseline incremental learning. In other words the model that is trained in a particular iteration, is already fine tuned with respect to a specific portion of the dataset. The results show that the model is able to utilize the information gained in the previous iterations.

As stated in Chapter 3, the main concern about the incremental learning with data pooling strategy is overfitting to data points which are collected in earlier iterations. Considering how prone the neural models are for overfitting, this would be an expected situation but the results show that the model does not overfit when it is trained incrementally with data pooling. Training error of the model decreases and stabilizes through epochs. It can be hypothesized that the regularization methods that are employed, are preventing the model to overfit and making the learning process stable. In order to confirm this, an experiment with different dropout probabilities is done and the model's behaviour in the incremental learning setting is reported. As it is stated before the model which is used in the previous experiment regarding incremental learning, is already fine tuned with the validation set by using supervised learning. In other words, the parameter configuration which performed best on validation set with supervised learning, is selected for the incremental learning experiments reported in \ref{table:incrementalQuora}. Therefore there is no risk of overfitting to the test set in this experiment even if the model is evaluated on the test set. The model selection process is already done so the test set can be used for analyzing the effect of different dropout probabilities. Table 4.3 shows the effect of different dropout probabilities on the incremental learning with data pooling. The models are evaluated on the test set.

As it can be seen from the table, the model is able to learn with the dropout probabilities of 0.3 and 0.5 while it fails to adapt when there is no dropout. Even though training error decreases steadily in every iteration, the model can not improve on the test set, overfitting over time which is the expected behaviour. The incremental learning baseline is also trained with different dropout probabilities. The results are shown in Table 4.4. As it can be seen from the table, the model manages to improve its' performance on test set even though no dropout is used.

\begin{table}[t]
\centering
\small
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  0 & \textbf{10.59} & 15.27 & 19.20 & 19.69 & 21.60 & 18.69 & 22.37 & 22.86 & 23.09  \\ 
 \hline
  0.3 & 7.93 & \textbf{15.37} & \textbf{17.58} & \textbf{19.73} & \textbf{21.20} & \textbf{21.63} & \textbf{22.49} & \textbf{23.12} & \textbf{23.43} \\ 
 \hline
  0.5 & 8.19 & 13.34 & 15.33 & 17.09 & 18.36 & 19.05 & 20.09 & 20.94 & 21.54 \\ 
 \hline
\end{tabular}
\caption{BLEU scores of incremental learning baseline on the QUORA dataset with different dropout probabilities. Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. First column represents the dropout probabilities. The models are evaluated on the test set.}
\end{table}

Incremental learning without data pooling achieves a BLEU score of 19.70 at the last iteration. The results show that even though the model improves its BLEU score in first 4 iterations it fails to adapt afterwards achieving similar BLEU scores for last 5 iterations. The model has the highest score in last iteration like the other learning strategies but increase in performance is not notable. Moreover at the last iteration, incremental learning without data pooling performs worse than both incremental learning baseline and incremental learning with data pooling.

\begin{table}[b]
\centering
\small
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  5 & 10.68 & 17.58 & 18.79 & \textbf{19.06} & \textbf{19.50} & \textbf{19.46} & \textbf{19.35} & \textbf{19.54} & \textbf{19.70}  \\ 
 \hline
  10 & 17.07 & \textbf{18.62} & \textbf{18.83} & 18.54 & 18.76 & 19.12 & 19.15 & 19.45 & 19.69 \\ 
 \hline
 15 & \textbf{18.96} & 18.07 & 18.47 & 18.46 & 18.87 & 19.05 & 19.11 & 19.15 & 19.19 \\ 
 \hline
\end{tabular}
\caption{BLEU scores of incremental learning without data pooling on the QUORA dataset with different number of epochs. Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. First column represents the number of epochs. The models are evaluated on the test set.}
\end{table}

At the end of the last iteration, both the baseline incremental learning and incremental learning without data pooling have models which are trained the same number of epochs. The baseline model is outperformed by the model trained with incremental learning without data pooling in the first 3 iterations but starting with the 4th iteration, it begins to outperform the other. This is an expected result because the continuous model has better weight initialization when there is not enough training data for the model to learn properly.

Both of the incremental learning strategies that are proposed, use transfer learning in the same dataset only difference being the resources they transfer from. Incremental learning with data pooling not only uses more training data to transfer between iterations, it also updates its' weights towards to certain parts of the datasets more. It can be argued that the reason behind the low performance of incremental learning without data pooling is the fact that it is not updating the model's weights enough, hence underfitting to the whole dataset. An experiment with different number of epochs per iteration is done in order to test this theory. The models are evaluated on the test set. The same argument from the experiments with different dropout probabilities, applies here as well. The results are shown in Table 4.5.

As it can be seen from the table, the number of epochs used in training does not effect the performance significantly. The model trained with number of epochs of 5, 10 and 15, scored BLEU scores of 19.70, 19.69 and 19.82 respectively at the last iteration. Training error analysis of the model clearly shows that the model is able to learn pretty fast since the training error continuously decreases and converges in every case. This might be attributed to size of the training data in each iteration considering the fact that the model can fit itself to the small dataset faster.

Since the model is not underfitting when it is trained with training data from separate iterations, low perplexity of the training clearly shows this, it would be insightful to look at other aspects of the model's behaviour. Another explanation of the low performance of incremental learning without data pooling could be ineffectiveness of knowledge transfer from the same dataset. It is important to point out that the notion of low performance indicates the model's inability to adapt rather than the BLEU score at the last iteration. There is a possibility that the model is completely unable to transfer the knowledge it gained from the previous iterations. In order to test this hypothesis, the model is trained in a supervised manner without data pooling and with reinitializing (no transfer between iterations). Table 4.6 shows the results. 

\begin{table}[t]
\centering
\small
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  BLEU & 10.15 & 12.87 & 12.65 & 16.03 & 15.82 & 16.19 & 16.62 & 16.77 & 16.35  \\ 
 \hline
\end{tabular}
\caption{BLEU scores of supervised learning without data pooling on QUORA dataset. Each column represents an iteration and each iteration trained separately.}
\end{table}

As it can be seen from the table, incremental learning performs better than supervised learning in every iteration. This shows that continually training the model and transferring the weights through iterations has an effect on model's performance. It can be easily seen that the model is able to use some part of the previous knowledge. From the analysis on incremental learning without data pooling, it can be hypothesized that even though the model is able to utilize past information, it gradually forgets it. In other words the model overwrites the weights which are fine tuned in previous iterations.

\section{Neural Paraphrase Generation with Transfer Learning}

\begin{table}[b]
\centering
\small
 \begin{tabular}{|c | c | c |}
 \hline
 \textbf{Transfer Method} & \multicolumn{2}{ c| }{\textbf{Source Dataset}} \\
 \hline
  & QUORA & PPDB \\ [0.5ex] 
 \hline
  INIT & 8.9 & \textbf{3.08}  \\ 
 \hline
  Freeze 1 layer & \textbf{10.04} & 1.73  \\ 
 \hline
  Freeze 2 layers & 8.74 & 2.46  \\ 
 \hline
  Surplus layer & 8.32 & 2.32  \\ 
 \hline
  Embedding only & 2.38 & 0.73  \\ 
 \hline
  Supervised Learning & 1.34 & 1.34  \\ 
 \hline
\end{tabular}
\caption{BLEU scores of different transfer learning methods on the MSR dataset. First row represents the source datasets. Rest of the rows represent transfer methods which are described in \ref{transfers} and corresponding BLEU scores.}
\end{table}

Table 4.7 presents the results of transfer learning on the MSR dataset. QUORA and PPDB-Lexical datasets are used as source datasets. Both source datasets are trained for 10 epochs and with the same hyperparameter configuration. No additional parameter tuning has been done for neither of the settings. Previously fined tuned model configurations which are described in previous section, are used for the target datasets.

As it can be seen from the table, transfer learning improves BLEU score with every transfer method for both source datasets except one particular setting. When transferred from the QUORA dataset, the model improves its BLEU score by 9.7 at the best case which is a very significant improvement since the original model could not produce acceptable paraphrases at all. When transferred from PPDB even though the model is still unable to generate acceptable paraphrases, it improves its BLEU score only by 1.74 at the best case which is an unexpected result. Our results on transfer learning from PPDB-Lexical dataset do not coincide with the results of \cite{brad} where they report a BLEU score of 10.29 for transfer from PPDB-Lexical to MSR. One possible explanation for this inconsistency would be the lack of parameter tuning for source datasets in our experiments. Since the source model for PPDB-Lexical is not fine tuned, it might not perform well in the MSR. Another explanation could be the difference in models used for paraphrase generation task but this explanation is very unlikely since our model is able to generate results that are comparable to state-of-the-art architectures even though it is a fairly simple model. In retrospect, the result that using QUORA dataset as source leading to better performance than using PPDB-Lexical, is not surprising. Even though the PPDB-Lexical dataset is larger, it contains no context information unlike the QUORA dataset. Moreover QUORA and MSR datasets have similar domains (more common words and topics, etc.) which should lead to better performance.

The transfer method which performs best for the QUORA to MSR transfer, is freeze 1 layer. When the QUORA dataset is used as source, rest of the transfer methods performs similarly, INIT improving the BLEU score by 6.52, freeze 2 layers improving by 6.36 and surplus layer by 5.98. For the PPDB-Lexical dataset, the best transfer method is INIT. Freeze 1 layer improves the BLEU score by 1, freeze 2 layers by 1.73 and surplus layer by 1.59. When only the trained word embeddings are transferred, the model improves by 1.04 with the QUORA dataset as source. This result shows that hidden layers are not the only aspect of the model that is transferable. When PPDB-Lexical dataset is used as source, transferring the word embeddings actually hurts the performance. This result is quite inconsistent with other transfer learning experiments and it lacks an explanation.

\begin{table}[t]
\centering
\small
 \begin{tabular}{|c | c | c |} 
  \hline
 \textbf{Transfer Method} & \multicolumn{2}{ c| }{\textbf{Target Dataset}} \\
 \hline
 & QUORA-120K & QUORA-24K \\ [0.5ex] 
 \hline
  INIT & 22.71 & 13.46  \\ 
 \hline
  Freeze 1 layer & 21.73 & \textbf{14.80}  \\ 
 \hline
  Freeze 2 layers & 21.56 & 12.93  \\ 
 \hline
  Surplus layer & 19.37 & 12.08  \\ 
 \hline
  Embedding only & 21.89 & 12.57  \\ 
 \hline
  Supervised Learning & \textbf{23.43} & 10.49  \\ 
 \hline
\end{tabular}
\caption{BLEU scores of different transfer learning methods on the QUORA-120K and the QUORA-24K datasets when MSCOCO dataset is used as source. First row represents the target datasets. Rest of the rows represent transfer methods which are described in \ref{transfers} and corresponding BLEU scores.}
\end{table}

Table 4.8 presents the results of transfer learning on the QUORA dataset from the MSCOCO dataset. Pre-trained source model shares the same hyperparameter configuration with previous source models and it is trained for 5 epochs. The results show that transferring MSCOCO to QUORA dataset is not effective, moreover it decreases the performance. All of the transfer methods perform worse than traditional supervised learning without transfer. Among the transfer methods only INIT has a comparable performance to supervised learning with a BLEU score of 22.71 whereas supervised learning achieves a BLEU score of 23.43. This is an unexpected result since it suggests that MSCOCO dataset has no useful knowledge which can help with the training of QUORA dataset. It can be argued that this is a similar problem to the knowledge forgetting problem which is observed with incremental learning without data pooling. It can be hypothesized that since there is enough training data to learn from, the transferred knowledge is overwritten by the target task. In order to investigate this behaviour better, a subset of QUORA dataset with the size of 24,000 paraphrase pairs are randomly selected (data from the first iteration in incremental learning setup) and knowledge transfer is applied from the MSCOCO to this subset. The original dataset contains approximately 120,000 paraphrase pairs. Results show that transfer learning performs better in this case, improving the BLEU score by 4.31. All of the transfer methods improve the model's final score and freeze 1 layer approach performs the best.

Transferring output layers makes no significant improvements to the performance for both QUORA and MSR target datasets.


\section{Neural Paraphrase Generation with Active Learning}

Table 4.9 presents the results of active learning on the QUORA dataset. In the table, RS, NGC and RNGC represent random sampling, n-gram coverage and reverse n-gram coverage respectively. Reverse n-gram which is the exact opposite of n-gram coverage is also experimented as a sampling strategy. Its score for a sentence is defined as 1 subtracted by n-gram coverage score of the sentence. The reason for this is to determine how well n-gram coverage reflects the informativeness of source paraphrases. As it can be seen from the table none of the sampling strategies is able to outperform random sampling. While random sampling achieves a BLEU score of 26.19 at the last iteration, n-gram and reverse n-gram coverage sampling achieve 23.69 and 22.43 respectively. Moreover they are unable to learn any faster than random sampling. Random sampling outperforms the others at all iterations except the first.

\begin{table}[t]
\centering
\small
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  RS & 10.49 &  \textbf{19.43} & \textbf{21.91} &  \textbf{23.30} &  \textbf{24.30} &  \textbf{24.50} &  \textbf{25.15} &  \textbf{25.45} &  \textbf{26.19}  \\ 
 \hline
  NGC &  \textbf{11.61} & 14.96 & 17.58 & 18.91 & 20.45 & 21.58 & 21.93 & 23.00 & 23.69 \\ 
 \hline
 RNGC & 5.71 & 14.76 & 19.35 & 20.46 & 21.10 & 21.79 & 22.70 & 22.14 & 22.43 \\ 
 \hline
\end{tabular}
\caption{BLEU scores of active learning on the QUORA dataset. Each column represents an iteration, first row shows what percent of the dataset is used in that iteration. Each row except the last represents the performance of a particular sampling strategy which is described in \ref{actives}.}
\end{table}

\section{Neural Paraphrase Generation with Network Expansion}

Table 4.10 presents the results of incremental learning with network expansion on QUORA dataset. In the table, NE represents the settings where the first active layer of the network is freezed after each expansion. NE2 represents the settings where no freezing is done. IL1 and IL2 represents incremental learning with and without data pooling, respectively. As it can be seen from the table, incremental learning with and without data pooling achieve BLEU scores of 25.05 and 15.83 at the last iteration.

In the case of incremental learning with data pooling, the model gradually increases its performance except when a new layer is added (iterations 4 and 7). On the other hand, in the case of Incremental learning without data pooling, the model improves until the first extra layer is added (iteration 4). After that point on, adaptiveness of the model diminishes almost completely.

\begin{table}[t]
\centering
\small
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  IL1-NE & \textbf{11.12} & 19.83 & 22.20 & 22.05 & 23.47 & 23.70 & 23.32 & 24.46 & 25.05  \\ 
 \hline
  IL2-NE & 10.14 & 17.23 & 17.77 & 16.00 & 16.23 & 16.40 & 16.06 & 16.42 & 15.83 \\ 
 \hline
  IL2-NE2 & 9.45 & 17.44 & 18.05 & 17.81 & 18.48 & 18.67 & 17.81 & 18.21 & 18.06 \\ 
 \hline
  IL1 & 10.49 &  \textbf{19.43} & \textbf{21.91} &  \textbf{23.30} &  \textbf{24.30} &  \textbf{24.50} &  \textbf{25.15} &  \textbf{25.45} &  \textbf{26.19}  \\ 
 \hline
  IL2 &  10.68 & 17.58 & 18.79 & 19.06 & 19.50 & 19.46 & 19.35 & 19.54 & 19.70 \\ 
 \hline
\end{tabular}
\caption{BLEU scores of incremental learning with and without network expansion (see \ref{incs} for details) on the QUORA dataset. Each column represents an iteration, first row shows what percent of the dataset is used in that iteration. IL2-NE2 represents incremental learning without data pooling and without layer freezing.}
\end{table}

When the model's layers are not freezed with network expansion, in incremental learning without data pooling, the model's behaviour does not change much, meaning it still loses its adaptiveness while its overall performance increases by 2.23.

\section{Combining Different Strategies}

\begin{table}[b]
\centering
\small
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  INIT & \textbf{5.23} & \textbf{6.28} & 6.90 & \textbf{8.31} & \textbf{8.97} & 8.93 & \textbf{10.39} & 10.70 & \textbf{11.20}  \\ 
 \hline
 Freeze 1 layer & 4.23 & 5.99 & 6.74 & 7.77 & 8.71 & \textbf{9.09} & 9.29 & 10.14 & 10.37 \\ 
 \hline
 Freeze 2 layers & 5.06 & 5.94 & 7.36 & 7.74 & 8.88 & 8.86 & 10.15 & \textbf{10.77} & 11.03 \\ 
  \hline
  Surplus layer & 1.44 & 3.61 & 3.86 & 4.74 & 4.89 & 5.37 & 5.12 & 6.12 & 6.57 \\ 
  \hline
  SL & - & - & - & - & - & - & - & - & 10.04 \\ 
 \hline
\end{tabular}
\caption{BLEU scores of incremental learning with data pooling trained with different transfer methods on the MSR dataset. Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. Last row represents supervised learning.}
\end{table}

Results of the experiments done with MSR dataset shows that the model fails to learn the dataset properly with incremental or supervised learning but the model is able to generate acceptable sentences when transfer learning is used. Since it is not possible to analyse the performance of incremental learning on MSR dataset as it is, the model is trained with both transfer and incremental learning. A source model is trained on another dataset and used as a knowledge base for target model. Target model is then trained incrementally using different transfer methods. With this experiment setup a concept drift situation is also simulated since a model that is trained on one particular dataset, is getting exposed to a different dataset. Table 4.11 presents the results of incremental learning with data pooling with different transfer methods. The QUORA dataset is used as source dataset in this experiment and for target model, the same hyperparameter configuration used in previous experiments (which is determined by using the validation set) is kept.

As it can be seen from the table, all the transfer methods steadily improve their performances through iterations which is an expected behaviour from previous experiments. Every transfer method except the surplus layer outperforms traditional supervised learning. Best performing transfer method is INIT with a BLEU score of 11.20 whereas supervised learning achieves 10.04. Freezing the first layer and first two layers achieves BLEU scores of 10.37 and 11.03 respectively. Additionally, the best transfer method outperforms supervised learning with only 80 percent of the dataset which is a result that is consistent with previous experiments.

\begin{table}[t]
\centering
\small
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  IL1 & \textbf{5.23} & \textbf{6.28} & \textbf{6.90} & \textbf{8.31} & \textbf{8.97} & \textbf{8.93} & \textbf{10.39} & \textbf{10.70} & \textbf{11.20}  \\ 
 \hline
  IL2 & 5.05 & 3.36 & 3.96 & 4.35 & 4.32 & 3.93 & 4.29 & 4.27 & 4.274 \\ 
 \hline
  IL3 & 5.09 & 5.90 & 6.43 & 7.57 & 8.52 & 8.79 & 9.86 & 9.98 & 10.02 \\ 
  \hline
  SL & - & - & - & - & - & - & - & - & 10.04 \\ 
 \hline
\end{tabular}
\caption{BLEU scores of different incremental learning strategies trained with INIT transfer method on the MSR dataset. Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. Last row represents supervised learning.}
\end{table}

Transferring by adding a surplus layer and freezing the rest does not work well, achieving a BLEU score of 6.57 underperforming even the supervised learning. Training error analysis of the method clearly shows why it is not performing well. Training error of the model with surplus layer stabilizes much slower and at a much larger value than other transfer methods. Moreover, it is observed that the paraphrases that are generated by the model during training, are dominated by the source dataset. In other words the model dominantly chooses words from the source dataset. At the end it is clear that the model underfits to the MSR dataset when it transfers with a surplus layer.

\begin{table}[b]
\centering
\footnotesize
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  INIT & 13.64 & 17.03 & 20.10 & 21.14 & 22.34 & 22.78 & 23.77 & 24.41 & 24.85  \\ 
 \hline
  Freeze 1 Layer &  \textbf{14.80} & 17.58 & 19.33 & 20.68 & 21.87 & 22.19 & 23.52 & 23.80 & 24.35 \\ 
  \hline
  IL1 & 10.49 &  \textbf{19.43} & \textbf{21.91} &  \textbf{23.30} &  \textbf{24.30} &  \textbf{24.50} &  \textbf{25.15} &  \textbf{25.45} &  \textbf{26.19}  \\ 
 \hline
\end{tabular}
\caption{BLEU scores of incremental learning with data pooling trained with transfer methods INIT and Freeze 1 layer on the QUORA dataset. Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. Last row represents incremental learning without transfer.}
\end{table}

Table 4.12 presents the results of transfer method INIT with different incremental learning strategies. QUORA dataset is used as source dataset in this experiment as well. 

As it can be seen from the table while incremental learning with data pooling and baseline incremental learning improve their performance through iterations, incremental learning without data pooling is unable to adapt. Results of incremental learning on MSR dataset is quite consistent with the results on QUORA dataset. Model's behaviour with different incremental learning strategies are similar in both datasets. Only notable difference between two sets of experiments is the underfitting in incremental learning without data pooling on MSR dataset. The model is unable to learn well on training data without pooling, producing large training errors during training. It can be hypothesized that reason for this is small size of the dataset and there is simply not enough data in one iteration for model to learn properly. 

Table 4.13 presents the results of incremental learning with data pooling with INIT and freeze 1 layer transfer methods. MSCOCO dataset is used as source and QUORA dataset is used as target. The same hyperparameter configuration from the previous experiments, is kept.

As it can be seen from the table, the model is able to gradually increase its performance in both cases. At the last iteration, the model achieves BLEU scores of 24.85 and 24.35 using transfer methods INIT and freeze 1 layer respectively. Results are consistent with previously reported transfer learning experiments where the model without transfer is outperforming the model with transfer. As reported in section 4.1.1, incremental learning with data pooling achieves a BLEU score of 26.19 without any transfer.  

\section{Discussion}

All the experiments which are conducted regarding incremental learning, show that incremental learning with data pooling has the best performance. Even though the baseline incremental learning (which is basically supervised learning) also increase its performance through time, the difference between BLEU scores clearly shows that continuously training the model leads to a better tuned and more data efficient model. Table 4.14 presents some selected paraphrases generated by the model on the QUORA dataset. As it can be seen from the table, the model is able to generate grammatically correct paraphrases with lexical variety but there are cases where it fails to capture the original meaning. On both datasets, incremental learning with data pooling is able to outperform traditional supervised learning without seeing 100 percent of the dataset. It is also important to notice the fact that baseline incremental learning and incremental learning with data pooling trains for same number of epochs. Therefore their total training times are approximately the same. 

Before the experiments, main concern regarding to the incremental learning was overfitting but the results show that overfitting is avoided because of the regularization mechanisms. Especially the dropout mechanism effectively prevents overfitting and makes incremental learning with data pooling possible. Once overfitting is removed from the equation (of course it is impossible to completely eliminate overfitting), it is not hard see why incremental learning with data pooling works better. By continuously training through iterations, the model starts the next iterations with better weight initializations and previous knowledge regarding the dataset. In other words transfer learning is applied between the same dataset (this is also correct for incremental learning without data pooling). Positive effect of incremental learning is evident if learning curve of the model is analyzed. During the experiments it was observed that for each iteration the training and validation errors in the beginning are lower than previous iterations. Another interesting aspect of this learning strategy is the amount of increase in BLEU score through iterations. Rate of performance gain is higher in the early iterations than late iterations on both datasets. One would expect similar increases in BLEU scores on the MSR dataset since it is a very hard dataset to paraphrase. Training and validation errors are much higher on MSR than on QUORA therefore there should be more room for the model to learn in the MSR (stabilizing in later iterations).

Incremental learning without data pooling does not perform well regarding both final BLEU score and adaptivity which is an expected outcome. The behaviour of this learning strategy was similar in all experiments (on both datasets, with network expansion, transfer learning and different configurations). The model starts improving in first half of the iterations and adaptivity severely diminishes after that point. The interesting finding is the fact that model learns separate iterations rather quickly but fails to improve its performance in test set which suggests catastrophic forgetting between iterations. In the end incremental learning without data pooling does not seem to be a suitable training strategy for paraphrase generation.

With the findings on incremental learning, it can be concluded that the answer to the first research question, "\textit{Can deep neural models effectively adapt and gradually perform in a continuous data stream for paraphrase generation?}", is yes. The results show that with incremental learning baseline and incremental learning with data pooling, a deep neural model can be successfully integrated into human-in-the-loop setting for paraphrase generation. The results also demonstrate the adaptivity of incremental learning with data pooling and its superiority over supervised learning and baseline incremental learning, in this particular setting. Moreover the second research question "\textit{Can the model achieve better or comparable performance than traditional supervised learning by leveraging the data stream?}" can also be partially answered since it is successfully shown that incremental learning learns better and faster on two different paraphrase datasets.

\begin{table}
\begin{adjustbox}{width=1\textwidth}
 \begin{tabular}{||c c c||} 
 \hline
 Label & Sentence & \\ [0.5ex] 
 \hline
 source & What are the causes of the fall of the roman empire? &  \\
 \hline
 target & What were the most important causes and effects of the fall of the roman empire? &  \\
 \hline
 generated & What are the causes of the collapse of roman empire? & \\
 \hline
 source & What are the best new car products or inventions that most people dont know about? &  \\
 \hline
 target & What are some mindblowing vehicles tools that exist that most people dont know about? & \\
 \hline
 generated & What are some mindblowing vehicle accessories that exist that most people don't know about? &  \\
 \hline
 source & How does a long distance relationship work? &  \\
 \hline
 target & How are long distance relationships maintained? &  \\
 \hline
 generated & How can I maintain my long distance relationship to the best of my ability? &  \\
 \hline
 source & What do you think is wrong with indian schools and education? &  \\
 \hline
 target & What is wrong with the current indian education? &  \\
 \hline
 generated & What do you think of indian school of education? &  \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Selected example paraphrase pairs from the QUORA dataset.}
\end{table}


Results of the experiments which are done regarding transfer learning are not conclusive. While transfer learning improves BLEU score for MSR dataset significantly in some cases, it does not work well for the QUORA dataset, even decreasing the performance. In the case of the MSR dataset, one of our findings is inconsistent with \cite{brad}, regarding the effect of transfer from PPDB-Lexical dataset. While they report significant improvements, our experiments showed little to none improvement and even decrease in performance when only word embeddings is transferred. One can argue that the reason for our findings is the poor performance of source model since no parameter tuning is done for the source models, but the difference between reported BLEU score is too high which makes this explanation highly unlikely. Our initial expectation of high performance from the QUORA as a source dataset, is confirmed to be right. It is assumed that the transfer would be successful since the two datasets are similar (similar domain, similar words etc.) and transfer from the QUORA resulted in a significant improvement in BLEU score. In the case of QUORA dataset, transfer from the MSCOCO only works if a small subset of the dataset is used as target. It can be concluded that when the model has access to enough data, transferred knowledge becomes irrelevant pretty quickly even though the MSCOCO dataset is almost twice as big as the QUORA dataset. It can be also seen that when transfer learning works, hidden states (network weights) are not the only aspect of source model that is being transferred. Word embeddings are also transferrable. 

Regarding to different methods, applying restrictions to the model during transfer does not work as intended. The transfer is usually more successful when the model is less restricted but performances of four different transfer methods are close to each other. Our findings agrees with \cite{mou}, showing that transfer learning for NLP tasks highly depends on the task, model and datasets. Since it is possible to create a model that is impossible to get created without transfer, our findings on transfer learning can partially answer the third research question, "\textit{What are the possible challenges/limitations introduced by system and model in this setting and what are the possible learning strategies we can use for dealing with these challenges?}". In a situation where there is a very small and challenging dataset and no way to obtain more data, transfer learning can be the solution depending on the task and the datasets.

Most interesting finding regarding transfer learning is; when incremental learning is combined with transfer learning, the idea of specialization does not work at all for LSTM based models for paraphrase generation. In our experiments it can be seen that the more transfer method restricts model's training the worse the performance when the model is continuously trained. Since the QUORA dataset is way larger than the MSR dataset, adding a new layer to the model and freezing rest of the network seemed to be a good idea since there was a large and fine tuned general model to transfer from. Making only one layer to specialize on the MSR dataset looks like a natural option but this transfer methods fails miserably by underfitting to the MSR dataset. If the transfer is possible for supervised learning, contrary to surplus layer, other transfer methods works well when they are trained with incremental learning with data pooling, learning faster and better than traditional supervised learning. Therefore our findings on incremental transfer learning completes the answer of our second research question.

Results of active learning show that proposed sampling strategies are not effective for neural paraphrase generation. It is possible that the poor performance of n-gram coverage might be related to the fact that no information from target paraphrases is used for sampling \cite{rubio}. Even though this is a possible explanation, the results of n-gram and reverse n-gram coverage clearly show that rarity of a particular sentence does not reflect that sentence's informativeness. Despite the fact that the two heuristics are exact opposite of each other, their performance is similar. This finding reinforces the idea that n-gram coverage is not a suitable to be used in a sampling strategy. 

In general, sampling strategies bias the training data by selecting specific paraphrases. This can lead to misrepresentation of the original training data distribution, resulting in poor performance especially the sampling strategy is not reflective of the informativeness. Random sampling does not have this problem since it randomly selects the training data. It can be hypothesized that this is why it performs better than other sampling strategies.

Basic idea behind network expansion during incremental learning was to introduce some flexibility to model and slow down the forgetting behaviour observed during training. This does not work as intended since expanding the network had no positive effect on neither of the incremental learning strategies. Expanding the network without freezing the other layers shows some promising results and needs more investigation.

Results of the experiments where a concept drift is simulated, show that the model is able to adapt to the new training data even if it is from a different data distribution. In both of the cases, incremental learning with data pooling is able to improve its performance through iterations. Every transfer learning method except the surplus layer, is able to facilitate this behaviour. On QUORA dataset, even though the model is not able to outperform the case where there is no concept drift, it is still able to create acceptable paraphrases. Moreover this underperforming behaviour is not only expected because of the very nature of concept drift, it is also consistent with the experimental results of transfer learning on QUORA dataset.



