\chapter{Results}\label{results}

This chapter reports the results of experiments done with proposed learning strategies and discusses the findings. We start with evaluating each strategy separately followed by experiments which evaluates combinations of different strategies. We end with a overall discussion of the results.

\begin{table}[t]
\centering
\large
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  IL1 & 0.1 &  \textbf{0.92} &  \textbf{0.94} &  \textbf{1.56} &  \textbf{1.13} &  \textbf{1.11} &  \textbf{1.26} &  \textbf{1.37} &  \textbf{1.41}  \\ 
 \hline
  IL2 & 0.07 & 0.15 & 0.21 & 0.47 & 0.77 & 0.99 & 1.04 & 1.19 & 1.16 \\ 
 \hline
 Baseline & \textbf{0.12} & 0.27 & 0.6 & 1.32 & 0.96 & 1.08 & 1.24 & 1.26 & 1.34 \\ 
 \hline
 SL & - & - & - & - & - & - & - & - & 0.09  \\ 
 \hline
\end{tabular}
\caption{BLEU scores of incremental learning on MSR dataset. Each column represents an iteration, first row shows what percent of the dataset is used in that iteration. Each row except the last represents the performance of a particular learning strategy which are described in Chapter 3. The last column represents the results from \cite{brad}.}
\label{table:4.1}
\end{table}

\section{Neural Paraphrase Generation with Incremental Learning}

Table 4.1 presents the results of incremental learning strategies on MSR dataset. The model is highly sensitive to parameter tuning, failing to converge with high learning rates. We train the model on a validation set with different hyperparameter configurations, mainly with different dropout probabilities and learning rates, and select the best performing model. As it can be seen from the table, the model performs very poorly achieving a BLEU score of 1.34 with traditional supervised learning. We related this poor performance to dataset's small size and high complexity. Moreover we also report a result from [Brad et al.] in order to reinforce the difficulty of MSR dataset. Their model achieved a BLEU score of 0.09 on MSR dataset with supervised learning. 

Incremental learning with and without data pooling achieves BLEU scores of 1.41 and 1.16 respectively. We see that all of the learning strategies including the baseline, seem to improve over time increasing its performance in a steady fashion in every iteration even though there are anomalies in baseline and incremental learning with data pooling, 4th iteration with 50 percent of the data. It is also observed that when trained with data pooling in an incremental manner, the model performs slightly better than supervised learning. Nevertheless the model fails to generate meaningful and grammatically correct paraphrases in every case.

Even though we can see a clear pattern through iterations the results of this experiment is not conclusive due to model's very poor performance. Moreover the high variance between different runs highly suggest that the results are not meaningful and the neural network is basically modelling noise. Each learning strategy has been run 3 times and the variance of the best performing learning strategy which is the incremental learning with data pooling, is 0.617. This a very high number compared to actual BLEU scores achieved by the model. 

At the end we can easily conclude that generating coherent paraphrases for MSR dataset is impractical if it is not impossible for our model. Moreover it is not applicable to human-in-the-loop setting since it basically produces noise in early iterations. This severely undermines the involvement of model with data acquisition process in other words the models in unable to make meaningful suggestions to the user.

\begin{table}[b]
\centering
\large
\begin{adjustbox}{width=1.2\textwidth,center=\textwidth} 
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  IL1 & 10.49 &  \textbf{19.43} & \textbf{21.91} &  \textbf{23.30} &  \textbf{24.30} &  \textbf{24.50} &  \textbf{25.15} &  \textbf{25.45} &  \textbf{26.19}  \\ 
 \hline
  IL2 &  \textbf{10.68} & 17.58 & 18.79 & 19.06 & 19.50 & 19.46 & 19.35 & 19.54 & 19.70 \\ 
 \hline
 Baseline & 7.93 & 15.37 & 17.58 & 19.73 & 21.20 & 21.63 & 22.49 & 23.12 & 23.43 \\ 
 \hline
 SL & - & - & - & - & - & - & - & - & 22.90  \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{BLEU scores of incremental learning on QUORA dataset. Each column represents an iteration, first row shows what percent of the dataset is used in that iteration. Each row except the last represents the performance of a particular learning strategy which are described in Chapter 3. The last column represents the results from \cite{Guptaetal}.}
\label{table:4.2}
\end{table}

Table 4.2 presents the results of incremental learning strategies on QUORA dataset. No hyperparameter tuning is done on the model during this experiment. As it can be seen from the table, in every case the model performs reasonably well at the end of last iteration. The baseline model with traditional supervised learning achieves a BLEU score of 23.43 and it is able to generate meaningful, grammatically correct paraphrases. We also report the results from \cite{Guptaetal}. Their model achieved a BLEU score of 22.9 in QUORA dataset with supervised learning. It is important to state that the training set used in this experiment is slightly larger than the reference work. Additionally a much larger test set is used in this work. Therefore it is not possible to make conclusive statements regarding the comparison of two models. Nevertheless the comparison at least shows that our model performs comparably to the other state-of-the-art approaches.

\begin{table}[t]
\centering
\large
\begin{adjustbox}{width=1.2\textwidth,center=\textwidth} 
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  0 & \textbf{10.68} & 19.13 & 20.45 & 20.83 & 20.44 & 20.47 & 20.78 & 20.31 & 21.11  \\ 
 \hline
  0.3 & 10.49 & \textbf{19.43} & \textbf{21.91} & \textbf{23.30} & \textbf{24.30} & \textbf{24.50} & \textbf{25.15} & \textbf{25.45} & \textbf{26.19} \\ 
 \hline
  0.5 & 8.48 & 17.05 & 19.62 & 21.58 & 22.93 & 23.26 & 23.92 & 24.58 & 25.09 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{BLEU scores of incremental learning with data pooling on QUORA dataset with different dropout probabilities.  Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. First column represents the dropout probabilities.}
\end{table}

Incremental learning with data pooling achieves a BLEU score of 26.19 at the last iteration, outperforming other strategies including supervised learning. Moreover it starts outperforming traditional supervised learning which uses all of the dataset, only with 60 percent of the dataset. The model starts adapting and improves its performance constantly through iteration. Even with using only half of the dataset, the model can provide reasonable paraphrases. 

The comparison between baseline incremental learning and incremental learning with data pooling indicates the reasons why data pooling performs better. From the perspective of individual iterations with both learning strategy, the model trains with same number of epochs and same amount of data. Only difference between to learning strategies is, in incremental learning with data pooling the model employs transfer learning inside the same dataset by not re-initializing its weight in every iteration contrary to baseline incremental learning. In other words the model that is trained in a particular iteration, is already fine tuned with respect to a specific portion of the dataset. The results show that we are able to utilize the information gained in previous iteration.

As stated in Chapter 3, the main concern about the incremental learning with data pooling strategy is overfitting to data points which are collected in earlier iterations. Considering how prone the neural models are for overfitting, this would be an expected situation but the results show that the model does not overfit when it is trained with incrementally with data pooling. Training error of the model decreases and stabilizes through epochs. We hypothesize that the regularization methods we use, are preventing the model to overfit and making the learning process stable. In order to confirm this we experiment with different dropout probabilities and report the model's behaviour in incremental learning setting. Table 4.3 shows the effect of different dropout probabilities on incremental learning with data pooling.

As it can be seen from the table, the model is able to learn with the dropout probabilities of 0.3 and 0.5 while it fails to adapt when there is no dropout. Even though training error decreases steadily in every iteration, the model can not improve on the test set, overfitting over time which is the expected behaviour. We also train the incremental learning baseline with different dropout probabilities. The results are shown in Table 4.4. As it can be seen from the table, the model does not overfit even though no dropout is used.

\begin{table}[t]
\centering
\large
\begin{adjustbox}{width=1.2\textwidth,center=\textwidth} 
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  0 & \textbf{10.59} & 15.27 & 19.20 & 19.69 & 21.60 & 18.69 & 22.37 & 22.86 & 23.09  \\ 
 \hline
  0.3 & 7.93 & \textbf{15.37} & \textbf{17.58} & \textbf{19.73} & \textbf{21.20} & \textbf{21.63} & \textbf{22.49} & \textbf{23.12} & \textbf{23.43} \\ 
 \hline
  0.5 & 8.19 & 13.34 & 15.33 & 17.09 & 18.36 & 19.05 & 20.09 & 20.94 & 21.54 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{BLEU scores of incremental learning baseline on QUORA dataset with different dropout probabilities. Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. First column represents the dropout probabilities.}
\end{table}

Incremental learning without data pooling achieves a BLEU score of 19.70 at the last iteration. The results show that even though the model improves its BLEU score in first 4 iterations it fails to adapt afterwards achieving similar BLEU scores for last 5 iterations. The model has the highest score in last iteration like the other learning strategies but increase in performance is not notable. Moreover at the last iteration, incremental learning without data pooling performs worse than both incremental learning baseline and incremental learning with data pooling.

\begin{table}[b]
\centering
\large
\begin{adjustbox}{width=1.2\textwidth,center=\textwidth} 
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  5 & 10.68 & 17.58 & 18.79 & \textbf{19.06} & \textbf{19.50} & \textbf{19.46} & \textbf{19.35} & \textbf{19.54} & \textbf{19.70}  \\ 
 \hline
  10 & 17.07 & \textbf{18.62} & \textbf{18.83} & 18.54 & 18.76 & 19.12 & 19.15 & 19.45 & 19.69 \\ 
 \hline
 15 & \textbf{18.96} & 18.07 & 18.47 & 18.46 & 18.87 & 19.05 & 19.11 & 19.15 & 19.19 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{BLEU scores of incremental learning without data pooling on QUORA dataset with different number of epochs. Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. First column represents the number of epochs.}
\end{table}

When compared to baseline incremental learning, it is easy to see that at the last iteration both models are trained the same number of epochs. The baseline model is outperformed by the model trained with incremental learning without data pooling in the first 3 iterations but starting with the 4th iteration it begins to outperform the other. This is an expected result because the continuous model has better weight initialization when there is not enough training data for the model to learn properly.

Both of the incremental learning strategies we propose uses transfer learning in the same dataset only difference being the resources they transfer from. Incremental learning with data pooling not only uses more training data to transfer between iterations, it also updates certain parts of the datasets more. It can be argued that the reason behind the low performance of incremental learning without data pooling is the fact that it is not updating the models weights enough hence underfitting to the whole dataset. We experiment with different number of epochs per iteration in order to test this theory. The results are shown in Table 4.5.

As it can be seen from the table, the number of epochs used in training does not effect the performance significantly. The model trained with number of epochs of 5, 10 and 15, scored BLEU scores of 19.70, 19.69 and 19.82 respectively at the last iteration. Training error analysis of the model clearly shows that the model is able to learn pretty fast since the training error continuously decreasing and converges in every case. This might be attributed to size of the training data in each iteration considering the fact that the model can fit itself to the small dataset faster.

Since the model is not underfitting when it is trained with training data from separate iterations, low perplexity of the training clearly shows this, it would be insightful to look at other aspects of the model's behaviour. Another explanation of the low performance of incremental learning without data pooling could be ineffectiveness of knowledge transfer from the same dataset. It is important to point out that the notion of low performance indicates the model's inability to adapt rather than the BLEU score at the last iteration. There is a possibility that the model is completely unable to transfer the knowledge it gained from the previous iterations. In order to test this hypothesis, we train the model in a supervised manner without data pooling and with re-initializing (no transfer between iterations). Table 4.6 shows the results. 

\begin{table}[t]
\centering
\large
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  BLEU & 10.15 & 12.87 & 12.65 & 16.03 & 15.82 & 16.19 & 16.62 & 16.77 & 16.35  \\ 
 \hline
\end{tabular}
\caption{BLEU scores of supervised learning without data pooling on QUORA dataset. Each column represents an iteration, first row shows how much percent of the dataset is used in that iteration. Each iteration trained separately.}
\end{table}

As it can be seen from the table, incremental learning performs better than supervised learning in every iteration. This shows that continually training the model and transferring the weights through iterations has an effect on model's performance. It can be easily seen that the model is able to use some part of previous knowledge. From the analysis on incremental learning without data pooling, we hypothesize that even though the model is able to utilize past information, it gradually forgets it. In other words the model overwrites the weights which are fine tuned in previous iterations.

\section{Neural Paraphrase Generation with Transfer Learning}

\begin{table}[t]
\centering
\large
 \begin{tabular}{|c | c | c |} 
 \hline
 Source Dataset & QUORA & PPDB \\ [0.5ex] 
 \hline
  INIT & 8.9 & \textbf{3.08}  \\ 
 \hline
  Freeze 1 layer & \textbf{10.04} & 1.73  \\ 
 \hline
  Freeze 2 layers & 8.74 & 2.46  \\ 
 \hline
  Surplus layer & 8.32 & 2.32  \\ 
 \hline
  Embedding only & 2.38 & 0.73  \\ 
 \hline
  Supervised Learning & 1.34 & 1.34  \\ 
 \hline
\end{tabular}
\caption{BLEU scores of different transfer learning methods on MSR dataset. First row represents the source datasets. Rest of the rows represents transfer methods and corresponding BLEU scores.}
\end{table}

Table 4.7 presents the results of transfer learning on MSR dataset. QUORA and PPDB-Lexical datasets are used as source datasets. Both source datasets are trained for 10 epochs and with same hyperparameter configuration. No additional parameter tuning is been done for neither of the settings.

As it can be seen from the table, transfer learning improves BLEU score with every transfer method except one for both source datasets. When transferred from QUORA dataset, the model improves its BLEU score by 9.7 at the best case which is a very significant improvement since the original model could not produce acceptable paraphrases at all. When transferred from PPDB even though the model is still unable to generate acceptable paraphrases, it improves its BLEU score only by 1.74 at the best case which is an unexpected result. Our results on transfer learning from PPDB-Lexical dataset do not coincide with the results of [Bali et al] where they report a BLEU score of 10.29 for transfer from PPDB-Lexical to MSR. One possible explanation for this inconsistency would be the lack of parameter tuning for source datasets in our experiments. Since the source model for PPDB-Lexical is not fine tuned, it might not perform well in MSR. Another explanation could be the difference in models used for paraphrase generation task but we find this very unlikely since our model is able to generate results that are comparable to state-of-the-art architectures in though it is a fairly simple model. In retrospect, the result that using QUORA dataset as source leading to better performance than using PPDB-Lexical, is not suprising. Even though PPDB-Lexical dataset is larger, it contains no context information unlike QUORA dataset. Moreover QUORA and MSR datasets have similar domains (more common words and topics, etc.) which should lead to better performance.

The transfer method which performs best for QUORA dataset is freeze 1 layer. When QUORA dataset used as source, rest of the transfer methods performs similarly, INIT improving the BLEU score by 6.52, freeze 2 layers improving by 6.36 and surplus layer by 5.98. For PPDB dataset, the best transfer method is INIT. Freeze 1 layer improves the BLEU score by 1, freeze 2 layers by 1.73 and surplus layer by 1.59. When only the trained word embeddings are transferred the model improves by 1.04 with QUORA dataset as source. This result shows that hidden layers are not the only aspect of the model that is transferable. When PPDB-Lexical dataset is used as source, transferring the word embeddings actually hurts the performance. This result is quite inconsistent with other transfer learning experiments and it lacks an explanation.

\begin{table}[t]
\centering
\large
 \begin{tabular}{|c | c | c |} 
 \hline
 Target Dataset & QUORA-120K & QUORA-24K \\ [0.5ex] 
 \hline
  INIT & 22.71 & 13.46  \\ 
 \hline
  Freeze 1 layer & 21.73 & \textbf{14.80}  \\ 
 \hline
  Freeze 2 layers & 21.56 & 12.93  \\ 
 \hline
  Surplus layer & 19.37 & 12.08  \\ 
 \hline
  Embedding only & 21.89 & 12.57  \\ 
 \hline
  Supervised Learning & \textbf{23.43} & 10.49  \\ 
 \hline
\end{tabular}
\caption{BLEU scores of different transfer learning methods on QUORA-120K and QUORA-48K datasets when MSCOCO dataset used as source. First row represents the target datasets. Rest of the rows represents transfer methods and corresponding BLEU scores.}
\end{table}

Table 4.8 presents the results of transfer learning on QUORA dataset from MSCOCO dataset. Pre-trained source model shares the same hyperparameter configuration with previous source models and it is trained for 10 epochs. The results show that transferring MSCOCO to QUORA dataset is not effective moreover it decreases the performance. All of the transfer methods performs worse than traditional supervised learning without transfer. Among the transfer methods only INIT has a comparable performance to supervised learning with a BLEU score of 22.21 whereas supervised learning achieves a BLEU score of 22.40. This is an unexpected result since it suggest that MSCOCO dataset has no useful knowledge which can help with the training of QUORA dataset. It can be argued that this is a similar problem to the knowledge forgetting problem which is observed with incremental learning without data pooling. We hypothesize that since there is enough training data to learn from the transferred knowledge is overwritten by the target task. In order to investigate this behaviour better, we randomly select a subset of QUORA dataset with the size of 24,000 paraphrase pairs and try knowledge transfer from MSCOCO to this subset. The original dataset contains approximately 120,000 paraphrase pairs. Results show that transfer learning performs better in this case, improving the BLEU score by 4.31. All of the transfer methods improve the model's final score and freeze 1 layer approach performs best.

Transferring output layers makes no significant improvements to the performance for both QUORA and MSR target datasets.


\section{Neural Paraphrase Generation with Active Learning}


\section{Neural Paraphrase Generation with Network Expansion}

Table 4.9 presents the results of incremental learning with network expansion on QUORA dataset. As it can be seen from the table, incremental learning with and without data pooling achieve BLEU scores of 21.26 and 14.77 at the last iteration.

In the case of incremental learning with data pooling, the model gradually increases its performance except when a new layer is added (iterations 4 and 7). On the other hand, in the case of Incremental learning without data pooling, the model improves until the first extra layer is added (iteration 4).  After that point on adaptiveness of the model diminishes almost completely.

When the model's layers are not freezed with network expansion, in incremental learning without data pooling, the model's behaviour does not change much, meaning it still loses its adaptiveness while its overall performance increases by 2.23.

\begin{table}[t]
\centering
\large
\begin{adjustbox}{width=1.2\textwidth,center=\textwidth} 
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  IL1-NE & \textbf{10.89} & 18.07 & 19.86 & 19.07 & 20.09 & 20.49 & 19.78 & 20.95 & 21.26  \\ 
 \hline
  IL2-NE & 10.14 & 15.86 & 16.77 & 15.00 & 15.55 & 15.70 & 15.03 & 15.42 & 14.77 \\ 
 \hline
  IL2-NE2 & 9.45 & 16.24 & 18.05 & 16.81 & 17.33 & 17.83 & 16.75 & 17.05 & 17.00 \\ 
 \hline
  IL1 & 10.49 &  \textbf{19.43} & \textbf{21.91} &  \textbf{23.30} &  \textbf{24.30} &  \textbf{24.50} &  \textbf{25.15} &  \textbf{25.45} &  \textbf{26.19}  \\ 
 \hline
  IL2 &  10.68 & 17.58 & 18.79 & 19.06 & 19.50 & 19.46 & 19.35 & 19.54 & 19.70 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{BLEU scores of incremental learning with and without network expansion on QUORA dataset. Each column represents an iteration, first row shows what percent of the dataset is used in that iteration. IL2-NE2 represents incremental learning without data pooling and layer freezing.}
\end{table}

\section{Combining Different Strategies}

Results of the experiments done with MSR dataset shows that the model fails to learn the dataset properly with incremental or supervised learning but the model is able to generate acceptable sentences while transfer learning is used. Since it is not possible to analyse the performance of incremental learning on MSR dataset as it is, we train the model with both transfer and incremental learning. A source model is trained on another dataset and used as a knowledge base for target model. Target model is then trained incrementally using different transfer methods. With this experiment setup we also simulate a concept drift situation since a model that is trained on one particular dataset, is getting exposed to a different dataset. Table 4.10 presents the results of incremental learning with data pooling with different transfer methods. QUORA dataset is used as source dataset in this experiment and for target model, we keep the same hyperparameter configuration which is determined by using the validation set, used in previous experiments.

\begin{table}[t]
\centering
\large
\begin{adjustbox}{width=1.2\textwidth,center=\textwidth} 
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  INIT & \textbf{5.23} & \textbf{6.28} & 6.90 & \textbf{8.31} & \textbf{8.97} & 8.93 & \textbf{10.39} & 10.70 & \textbf{11.20}  \\ 
 \hline
 Freeze 1 layer & 4.23 & 5.99 & 6.74 & 7.77 & 8.71 & \textbf{9.09} & 9.29 & 10.14 & 10.37 \\ 
 \hline
 Freeze 2 layers & 5.06 & 5.94 & 7.36 & 7.74 & 8.88 & 8.86 & 10.15 & \textbf{10.77} & 11.03 \\ 
  \hline
  Surplus layer & 1.44 & 3.61 & 3.86 & 4.74 & 4.89 & 5.37 & 5.12 & 6.12 & 6.57 \\ 
  \hline
  SL & - & - & - & - & - & - & - & - & 10.04 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{BLEU scores of incremental learning with data pooling trained with different transfer methods on MSR dataset. Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. Last row represents supervised learning.}
\end{table}

As it can be seen from the table, all the transfer methods steadily improve their performances through iterations which is an expected behaviour from previous experiments. Every transfer method except the surplus layer outperforms traditional supervised learning. Best performing transfer method is INIT with a BLEU score of 11.20 whereas supervised learning achieves 10.04. Freezing the first layer and first two layers achieves BLEU scores of 10.37 and 11.03 respectively. Additionally, the best transfer method outperforms supervised learning with only 80 percent of the dataset which is a results that is consistent with previous experiments.

Transferring by adding a surplus layer and freezing the rest does not work well, achieving a BLEU score of 6.57 underperforming even the supervised learning. Training error analysis of the method clearly shows why it is not performing well. Training error of the model with surplus layer stabilizes much slower and at a much larger value than other transfer methods. Moreover, it is observed that the paraphrases that are generated by the model during training, are dominated by the source dataset. In other words the model dominantly chooses words from the source dataset. At the end it is clear that the model underfits to MSR dataset when it transfers with a surplus layer.

\begin{table}[t]
\centering
\large
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  IL1 & \textbf{5.23} & \textbf{6.28} & \textbf{6.90} & \textbf{8.31} & \textbf{8.97} & \textbf{8.93} & \textbf{10.39} & \textbf{10.70} & \textbf{11.20}  \\ 
 \hline
  IL2 & 5.05 & 3.36 & 3.96 & 4.35 & 4.32 & 3.93 & 4.29 & 4.27 & 4.274 \\ 
 \hline
  IL3 & 5.09 & 5.90 & 6.43 & 7.57 & 8.52 & 8.79 & 9.86 & 9.98 & 10.02 \\ 
  \hline
  SL & - & - & - & - & - & - & - & - & 10.04 \\ 
 \hline
\end{tabular}
\caption{BLEU scores of different incremental learning strategies trained with INIT transfer method on MSR dataset. Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. Last row represents supervised learning.}
\end{table}

Table 4.11 presents the results of transfer method INIT with different incremental learning strategies. QUORA dataset is used as source dataset in this experiment as well. 

As it can be seen from the table while incremental learning with data pooling and baseline incremental learning improves their performance through iterations, incremental learning without data pooling is unable to adapt. Results of incremental learning on MSR dataset is quite consistent with the results on QUORA dataset. Model's behaviour with different incremental learning strategies are similar in both datasets. Only notable difference between two sets of experiments is the underfitting in incremental learning without data pooling on MSR dataset. The model is unable to learn well on training data without pooiling, producing large training errors during training. We hypothesize that reason for this is small size of the dataset and there is simply not enough data in one iteration for model to learn properly. 

\begin{table}[b]
\centering
\large
\begin{adjustbox}{width=1.2\textwidth,center=\textwidth} 
 \begin{tabular}{|c | c | c | c | c | c | c | c | c | c |} 
 \hline
 \% & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\ [0.5ex] 
 \hline
  INIT & 13.64 & 17.03 & 20.10 & 21.14 & 22.34 & 22.78 & 23.77 & 24.41 & 24.85  \\ 
 \hline
  Freeze 1 layer &  \textbf{14.80} & 17.58 & 19.33 & 20.68 & 21.87 & 22.19 & 23.52 & 23.80 & 24.35 \\ 
  \hline
  IL1 & 10.49 &  \textbf{19.43} & \textbf{21.91} &  \textbf{23.30} &  \textbf{24.30} &  \textbf{24.50} &  \textbf{25.15} &  \textbf{25.45} &  \textbf{26.19}  \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{BLEU scores of incremental learning with data pooling trained with transfer methods INIT and Freeze 1 layer on QUORA dataset. Each column except the first represents an iteration, first row shows what percent of the dataset is used in that iteration. Last row represents incremental learning without transfer.}
\end{table}

Table 4.12 presents the results of incremental learning with data pooling with INIT and freeze 1 layer transfer methods. MSCOCO dataset is used as source and QUORA dataset is used as target. We keep the same hyperparameter configuration as previous experiments since no parameter tuning is done for QUORA dataset.

As it can be seen from the table the model is able to gradually increase its performance in both cases. At the last iteration, model achieves BLEU scores of 24.85 and 24.35 using transfer methods INIT and freeze 1 layer respectively. Results are consistent with previously reported transfer learning experiments where the model without transfer is outperforming the model with transfer. As reported in section 4.1.1, incremental learning with data pooling achieves a BLEU score of 26.19 without any transfer.  

\section{Discussion}

All the experiments which are conducted regarding incremental learning, show that incremental learning with data pooling has the best performance. Even though the baseline incremental learning (which is basically supervised learning) also increase its performance through time, the difference between BLEU scores clearly shows that continuously training the model leads to a better tuned and more data efficient model. On both datasets, incremental learning with data pooling is able to outperform traditional supervised learning without seeing 100 percent of the dataset. It is also important to notice the fact that baseline incremental learning and incremental learning with data pooling trains for same number of epochs. Therefore their total training times are approximately the same. 

Before the experiments main concern regarding to incremental learning was overfitting but the results show that overfitting is avoided because of the regularization mechanisms. Especially dropout mechanism effectively prevents overfitting and makes incremental learning with data pooling possible. Once overfitting is removed from the equation (of course it is impossible to completely eliminate overfitting), it is not hard see why incremental learning with data pooling works better. By continuously training through iterations, the model starts the next iterations with better weight initializations and previous knowledge regarding the dataset. In other words we apply transfer learning between the same dataset (this is also correct for incremental learning without data pooling). Positive effect of incremental learning is evident if learning curve of the model is analyzed. During the experiments we observed that for each iteration the training and validation errors in the beginning are lower than previous iterations. Another interesting aspect of this learning strategy is the amount of increase in BLEU score through iterations. Rate of performance gain is higher in the early iterations than late iterations on both datasets. One would expect similar increases in BLEU scores in MSR dataset since it is a very hard dataset to paraphrase. Training and validation errors are much higher on MSR than on QUORA therefore there should be more room for the model to learn in MSR (stabilizing in later iterations).

Incremental learning without data pooling does not perform well regarding both final BLEU score and adaptivity which is an expected outcome. The behaviour of this learning strategy was similar in all experiments (on both datasets, with network expansion, transfer learning and different configurations). The model starts improving in first half of the iterations and adaptivity severely diminishes after that point. The interesting finding is the fact that model learns separate iterations rather quickly but fails to improve its performance in test set which suggests catastrophic forgetting between iterations. In the end incremental learning without data pooling does not seem to be a suitable training strategy for paraphrase generation.

With our findings on incremental learning, we can conclude that the answer to our first research question, "\textit{Can deep neural models effectively adapt and gradually perform in a continuous data stream for paraphrase generation?}", is yes. We show that with incremental learning baseline and incremental learning with data pooling, we can successfully integrate a deep neural model into human-in-the-loop setting for paraphrase generation. We demonstrate the adaptivity of incremental learning with data pooling and its superiority over supervised learning and baseline incremental learning, in this particular setting. Moreover we can also partially answer our second research question "\textit{Can we achieve better or comparable performance than traditional supervised learning by leveraging the data stream?}" since we successfully show that incremental learning learns better and faster on two different paraphrase datasets.


Results of experiments which are done regarding transfer learning is not conclusive. While transfer learning improves BLEU score for MSR dataset significantly in some cases, it does not work well for QUORA dataset even decreasing the performance. In the case of MSR, one of our findings are inconsistent with \cite{brad} regarding the effect of transfer from PPDB-Lexical dataset. While they report significant improvements, our experiments showed little to none improvement and even decrease in performance when we only transfer word embeddings. One can argue that the reason for our findings is the poor performance of source model since we do not do parameter tuning on source models, but the difference between reported BLEU score is too high which makes this explanation highly unlikely. Our initial expectation of high performance from QUORA as a source dataset, is confirmed to be right. We assumed that the transfer would be successful since the two datasets are similar (similar domain, similar words etc.) and transfer from QUORA resulted in a significant improvement in BLEU score. In the case of QUORA dataset, transfer from MSCOCO only works if we have a small subset of the dataset. We conclude that when the model has access to enough data, transferred knowledge becomes irrelevant pretty quickly even though MSCOCO dataset is almost twice as big as QUORA. We also see that when transfer learning works, hidden states (network weights) are not the only aspect of source model that is being transferred. Word embeddings are also transferrable. 

Regarding to different methods, applying restrictions to the model during transfer does not work as intented. The transfer is usually more successful when the model is less restricted but performances of four different transfer methods are close to each other. Our findings agrees with \cite{mou}, shows that transfer learning for NLP tasks highly depends on the task, model and datasets. Since we are able to create a model that is impossible to get created without transfer, our findings on transfer learning can partially answer our third research question, "\textit{What are the possible challenges/limitations introduced by system and model in this setting and what are the possible learning strategies we can use for dealing with these challenges?}". In a situation in which there is a very small and challenging dataset and no way to obtain more data, transfer learning can be the solution depending on the task and the datasets.

Most interesting finding regarding transfer learning is; when incremental learning is combined with transfer learning, the idea of specialization does not work at all for LSTM based models for paraphrase generation. In our experiments we have seen that the more transfer method restricts model's training the worse the performance when the model is continuously trained. Since QUORA dataset is way larger than MSR dataset, adding a new layer to the model and freezing rest of the network seemed to be a good idea since we have a large and fine tuned general model to transfer from. Making only one layer to specialize on MSR dataset looks like a natural option but this transfer methods fails miserably by underfitting to MSR dataset. If the transfer is possible for supervised learning, contrary to surplus layer other transfer methods works well when they are trained with incremental learning with data pooling, learning faster and better than traditional supervised learning. Therefore our findings on incremental transfer learning completes the answer of our second research question.


Basic idea behind network expansion during incremental learning was to introduce some flexibility to model and slow down the forgetting behaviour observed during training. This does not work as intented since expanding the network had no positive effect on neither of the incremental learning strategies. Expanding the network without freezing the other layers shows some promising results and needs investigating.





