\chapter{Conclusion}\label{conclusion}

This thesis studies neural paraphrase generation in settings where the training data is collected from a continuous data stream. A continuous, human-in-the-loop data acquisition process is simulated with existing datasets. A data driven and adaptive learning paradigm is proposed for training the model. A deep neural model based on stacked LSTM's is implemented and its behaviour is analyzed under different learning strategies. It is shown that integration of the model with the data stream is possible and it improves the performance of paraphrase generation task. Since the research on adaptive neural models for NLP tasks is quite limited, actual applicability of the concept is the main focus of this thesis. The work in this thesis can be considered as a proof of concept regarding adaptive models in continuous, human-in-the-loop settings.

Three different incremental learning strategies (including an incremental learning baseline to make meaningful comparisons) are proposed and experimented with, in order to show the model's ability to work in human-in-the-loop settings. Experiments with two different datasets have been conducted using iterative evaluation to monitor the learning process. Although one of the incremental learning strategies underperforms and fails to adapt through time, the other strategy is shown to be quite successful regarding both adaptivity and overall performance. Experiments also demonstrate that the successful incremental learning strategy outperforms the baseline with less amount of data. At the end, the practicality of adaptive learning over supervised learning is clearly demonstrated by the experiments.

For the cases where large amount of training data is not available, the effect of transfer learning on paraphrase generation is investigated. The possibility of knowledge transfer for neural paraphrase generation is questioned and an analysis is done in order to determine what aspects of the network are being transferred. It is shown that for paraphrase generation, the transfer is successful when the target dataset is low resource. It has a negative effect on performance when the target dataset is large. It is also shown that while the transfer of the output layers does not make any difference, both hidden layer weights and word embedding weights have an impact on performance during transfer. Additionally, four different transfer methods are proposed to be employed in different specific use cases. Initial hypothesises which are considered during the design of transfer methods fail since the experiments show that restricting the model's learning ability during transfer leads to poor performance.

In order to utilize the human-in-the-loop setting better by selecting the best samples to be annotated, three different active learning strategies for are proposed. It is clear from the experimental results that n-gram coverage of a sentence does not represent the informativeness of that sentence in paraphrase generation.

Finally the proposed learning strategies are tested in a setting where a concept drift occurs. A previously trained model encounters new training data which does not come from the original dataset the model trained with. Proposed learning strategies are combined in order to deal with this problem. The results show that in the case of a concept drift, the model is able to adapt to the new training data with incremental learning with data pooling. 

