\chapter{Introduction}\label{introduction}

Generally, machine learning applications build models depending on training data which is gathered collectively by using a specific data acquisition tool. This process introduces a number of problems regardless of quality and amount of the gathered training data. First of all, the data acquisition process itself can cost lots of time and money since the data has to be labelled by human workers. Moreover if the domain in which training data is collected from is very specific for example, medical text, certain number of domain experts might be needed. In the cases where the domain experts can not be utilised, gathering necessary amount of data to build a model can be impractical or impossible. Secondly, training on previously gathered large datasets with traditional supervised learning can be very computationally expensive. Thirdly and most importantly, data distribution of the domain where target application operates can change which leads to existing training data to become outdated. Since the model is not trained with changed data coming from the environment, it performs poorly. This phenomenon is called concept drift in the literature \cite{concept}. In this case more training data has to be gathered and the model has to be trained again with new data both of which can be again impractical or impossible depending on the resources at hand.

These mentioned problems make us to think about a data acquisition process where training data is collected interactively and iteratively with a model which adapts to the continuous data stream. In the proposed design, the model is trained continuously with each iteration of data acquisition process instead of training by traditional supervised learning with large amount of data. The model not only aims to automatically adapt to new training data without any retraining from scratch, it also interacts with human users by providing suggestions to users and making use of user feedback as a form of input to train. With this human-in-the-loop paradigm, necessity of collecting a large dataset in advance is eliminated and a functional model (even though it is not fully optimal to use) is created very fast, at the same time. We call this paradigm adaptive learning. Figure 1.1 shows the basic workflow of suggested data acquisition process. We aim to create a model which has comparable or better performance than a model trained with traditional supervised learning at the end of data acquisition process. Figure 1.2 shows a real world human-in-the-loop data acquisition system for text simplification.

\begin{figure}[h]
\includegraphics[width=\textwidth, height=10cm]{hil}
\centering
\caption{Human in the loop data acquisition}
\end{figure}

Main focus of this thesis is the behaviour of deep neural models in human-in-the-loop settings which is described above. We mainly study different learning strategies for training deep neural models in this specific setting, conduct experiments with different training methods to investigate their effectiveness in continuous data streams. We also propose and explore additional learning strategies to overcome some of the problems which are introduced by human-in-the-loop setting. Although human-in-the-loop data acquisition can help with the problems which are described in the beginning of this section, it also introduces some other problems regarding in the context of modelling. We start with proving that a deep neural model in fact can adapt and improve with the continuously increasing data and move to experiment with different learning strategies each of which addressing a different problem in human-in-the-loop setting. We study incremental learning instead of traditional supervised learning, in order to obtain adaptivity to incoming data without forgetting previous knowledge. We experiment with different ways to train the model continuously mainly related to how the model processes incoming data. We also show that with incremental learning we can create a relatively good and usable model, way before we are exposed to all of the training data. We experiment with transfer learning in order to simulate a concept drift scenario where a general model comes across to new training data with different nature. We also simultaneously simulate a situation in which we do not have enough resources to collect significant amount of data. We show that with transfer learning, it is possible to create model which is impossible to create with available data. Additionally we study different active learning strategies in order to reduce the data we need to build a satisfactory model. At the end we also do experiments in which we combine these strategies together and report the results.

\begin{figure}[h]
\includegraphics[width=\textwidth]{human-in-the-loop}
\centering
\caption{An interactive human-in-the-loop application for text simplification \cite{par4sim}. Target words to be simplified are highlighted and user is provided with a list of candidate words to choose from. Candidate words are generated and ranked by the system/model.}
\end{figure}


As the target application, we choose paraphrase generation. Paraphrase generation is the problem of generating different texts from a source text while retaining the meaning. It has various application areas in Natural Language Processing (NLP) like dialogue systems where it is used for building more natural conversional agents, information retrieval where it is used for enhancing retrieval process, natural language generation where it is used for generating training data for different NLP tasks and text summarization where it is used for replacing a text with a simpler paraphrase of it. Generating the text "President Trump strongly denied any wrong doing in this matter." from the text "POTUS firmly denied any fault in this subject." would be a good example of paraphrase generation. It is a challenging problem because of many different reasons. There are more than one way to paraphrase a source sentence and the quality of generated paraphrase depends on many things like context information, lexical and semantic diversity and so on. Therefore the problem is not only concerned with language generation, it also deals with language understanding. Another problem which is introduced by picking paraphrase generation as our target application is out of vocabulary words. Deep neural models for NLP problems is built on predefined and specific vocabularies which are usually constructed from training data. These vocabularies cannot be changed or expanded during or after training. Since the model does not know the words which it have not seen in the training, unknown words can lead to poor performance. This is an open problem in NLP and all neural models suffer from it but in our case it is more problematic because of the setting we study and nature of the problem we deal with. In our setting data comes to the system continuously and we train the model continuously therefore adapting model strictly depends on the vocabulary which it started training with. Additionally paraphrase generation is not a classification or regression task, it is a generation task therefore we do not know target texts (paraphrases) for our source texts beforehand, they are generated by users. These two mentioned reasons cause our model to be more prone to out of vocabulary words since there will be more of them compared to traditional supervised setting. There are two possible solutions for this problem. First solution is to use pre-trained word embeddings for our neural net instead of training our own word embeddings. These pre-trained word embeddings are built on very large corpuses so they would cover a lot of out of vocabulary words for our training data. Second solution is to design a data acquisition process similar to \cite{par4sim} where the system suggests paraphrase candidates which are created from large, existing resources and make the user choose from suggestions. Even though this puts restrictions to the users, it ensures that the target paraphrases are built from a known vocabulary. Since we know the vocabulary beforehand we could train our own word embeddings. In this thesis we simulate the latter solution. 

We simulate the human-in-the-loop described in this section by dividing a large existing dataset into subsets and feed the model with these subsets iteratively. In each iteration, the model gets a new batch of data and updates itself according to it. We train the model with these batches using different learning strategies and evaluate it on a separate test set at the end of each iteration, observing the model's behaviour through time.

According to our knowledge there is no work studying neural paraphrase generation in human-in-the-loop settings especially concerning adaptivity of the neural model. Moreover this work seems to be first to try incremental learning and active learning for neural paraphrase generation. We also build on to existing transfer learning research for neural paraphrase generation by experimenting with different methods of transfer and analysing the transfer process.

\section{Thesis Organization}
This thesis is organized as follows; the rest of this chapter describes the research question we would like to investigate and review state-of-the-art works in neural paraphrase generation, human-in-the-loop learning and the strategies we use in our work. In Chapter 2, we give the main methodology and technical details regarding our deep neural model and main ideas behind the strategies we use. Chapter 3 describes the details of our learning strategies, the datasets and evaluation metric we use. It also explains the experimental setups we have in detail with their corresponding reasonings. In Chapter 4 we present our results of the experiments and explain the findings. The last chapter gives a discussion on the overall findings of thesis, an extensive list of future work and the conclusion.

\section{Research Questions}

This thesis is going to aim to investigate whether deep neural models can be combined with adaptive learning, human-in-the-loop paradigm for paraphrase generation task. The basic research questions can be summarized as below:

\begin{itemize}
  \item Can deep neural models effectively adapt and gradually perform in a continuous data stream for paraphrase generation?

Considering the fact the deep neural models need a lot of training data to perform well, can we integrate this type of models into human-in-the-loop setting? We would like our model to learn and adapt continuously since we do not want to retrain large models over and over again through time. We would like to see that if such models can be applied to real world applications which have similar setting to ours. If the answer is yes, what should be the training scheme (how to make use of incoming data stream) for such models? Basically we would like to know if continuous learning is possible for neural paraphrase generation. 
 
  \item Can we achieve better or comparable performance than traditional supervised learning by leveraging the data stream?
  
  Can adaptive learning improve accuracy by adding more generalization power? Can the model achieve better or comparable performance with less data? It is also possible that training a deep neural network in adaptive manner may have a negative effect, if so it will be insightful to understand the reason why. Understanding how deep models behave under this training paradigm is an interesting and challenging question.
   
  \item What are the possible challenges/limitations introduced by system and model in this setting and what are the possible learning strategies we can use for dealing with these challenges?
  
  We would like to know what kind of existing learning strategies we can use to perform better in human-in-the-loop setting. This is particularly important since the setting itself imposes some challenges. How can we deal with challenges like limited resources and concept drift? Additionally training deep neural models continuously introduces challenges of its own like overfitting and underfitting depending on the training data.

\end{itemize}

Considering the lack of research in this area, with these questions answered, it might be possible to enhance the performance of deep neural models (shorter training time and / or better paraphrases) in paraphrase generation task and create a framework on adaptive learning for future research.

\section{Related Work}

Deep learning based approaches have been quite successful in various NLP tasks including language modelling \cite{siriam}, automatic speech recognition \cite{hannun} and neural machine translation \cite{cho}. Main idea behind deep learning is learning future representations of the dataset instead of depending on hand crafted features engineered by domain experts. Deep neural models is capable of hidden representations and relationships inside the data which are not possible to obtain by feature engineering. In the case of NLP, deep neural models are capable of capturing lexical, semantical and contextual relationships between textual data points which makes them quite successful. Nowadays, almost all of the state-of-the-art models used in NLP applications are based on deep learning.

For the task of paraphrase generation deep learning is already proved to be extremely successful by the work which has been done in past few years. Especially with the development of sequence to sequence learning \cite{Vinyalsetal}, a lot of research has been emerged, building models for paraphrase generation using this framework. \cite{Prakashetal} introduces a sequence to sequence model based on stacked LSTM's with residual connections. The authors use residual connections between stacked LSTM networks in order to address exploding/vanishing gradient problems for LSTM based models. They have experimented on different large scale datasets to show the effectiveness of their model which can produce meaningful, semantically and grammatically correct paraphrases. They also show that with residual connections, the LSTM based model can learn how to retain important words better. We use a lightweight variant of this model in this thesis.

\cite{Guptaetal} uses deep generative models coupled with LSTM's in sequence to sequence framework, in order to generate paraphrases. They modify the model by conditioning generative model with source sentence in order to enhance performance. They have managed to outperform state-of-the-art methods for paraphrase generation and generate a baseline for Quora Question Pair dataset which we use in this thesis. Additionally they evaluate their model with human evaluation which suggest the model can create good and relevant paraphrases. This work is particularly interesting since the resulting model can also link unseen concepts which are related to the original sentences.

\cite{Lietal} uses a very interesting approach, using recently popularized deep reinforcement learning for paraphrase generation. Their approach uses two components, a generator which is a sequence to sequence model, responsible for creating paraphrases, and an evaluator which is a deep matching model, responsible for recognizing paraphrases. Main idea is the evaluator enhancing the performance of generator. They propose this framework as a solution for lack of evaluation metrics for paraphrase generation. Their approach outperforms the state-of-the-art methods in paraphrase generation in both human evaluation and automatic evaluation.

Interactive data acquisition tools for NLP tasks has been worked on quite extensively in recent years. Many different tools are designed and implemented in order to enhance the ability to collect and use complex text data. They make use of multiple visualizations, correction mechanisms and machine learning based feedback loops. \cite{trivedi} developed an interactive user interface for enabling NLP tasks on clinical text which requires domain expertise. \cite{Yimam:2016aa} developed a tool for applying interactive, human-in-the-loop machine learning to biomedical entity and relation recognition. They demonstrate the effectiveness of their approach by simulating human-in-the-loop data acquisition with three experiments and prove their concept for adaptive learning. Following this work, \cite{par4sim} creates an adaptive learning system for text simplification which is integrated with a machine learning model. The system effectively makes use of the model in the data acquisition process, creating a feedback loop with human workers. They have done a real-time data acquisition using the Amazon MTurk crowdsourcing platform. They showed successful integration of their model and its adaptiveness through time by evaluating the model's performance and showing its improvement over time. This thesis is inspired by this particular work, replaces the model used with a deep neural model and studies its behaviour. For the adaptive, continual learning in neural networks \cite{parisi} gives an extensive review on the subject, covering different number of techniques (some of them are utilized in this thesis) to achieve adaptive learning. In this thesis, we study adaptive learning in the context of both applicability in real world and learning capability.

In case of transfer learning, it has been already established that knowledge transfer is possible and quite effective in computer vision. Recently, there has been substantial work on its possible applicability on the field of NLP also. An extensive look on knowledge transfer on different NLP tasks is provided in \cite{mou}. They do systematic experiments on different NLP problems to determine what kind of aspects of natural language and network layers are transferable. They show that knowledge transfer is possible between different datasets of same task and semantically similar tasks even though improvement from transfer learning depends on the datasets and the tasks at hand. \cite{zoph} successfully applies transfer learning to neural machine translation, gaining significant increase in performance for translation of low-resource languages. \cite{brad} uses transfer learning for neural paraphrase generation, effectively boosting the performance. Even though they successfully show that knowledge transfer is possible for neural paraphrase generation they do not study different ways to transfer. In our thesis, we conduct a layer-by-layer analysis for studying different transfer methods and what part of the network is being transferred. Finally, \cite{yoon} studies different transfer schemes for creating personalized language model from a general model. Building on these previous works, we provide a detailed investigation of transfer learning in neural paraphrase generation.

For active learning, as far as our knowledge is concerned there is no previous work. \cite{shen} successfully applies active learning for name entity recognition based on different sampling strategies. \cite{zhao} uses a sampling strategy based on a combination of model uncertainty and representativeness of data points, applying it on short-text classification. \cite{rubio} successfully applies it on interactive machine translation. Even though it has never been applied on paraphrase generation, these works show that active learning can be used for a variety of NLP tasks. We use some of the sampling strategies used in these works and try to apply them to paraphrase generation.